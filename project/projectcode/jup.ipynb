{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "\t\"\"\" sigmoid function \"\"\"\n",
    "\treturn 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def grad_g(x):\n",
    "\t\"\"\" gradient of sigmoid function \"\"\"\n",
    "\tgx = g(x)\n",
    "\treturn gx * (1.0 - gx)\t\n",
    "\n",
    "\n",
    "def predict(Theta1, Theta2, X):\n",
    "\t\"\"\" Predict labels in a trained three layer classification network.\n",
    "\tInput:\n",
    "\t  Theta1       trained weights applied to 1st layer (hidden_layer_size x input_layer_size+1)\n",
    "\t  Theta2       trained weights applied to 2nd layer (num_labels x hidden_layer_size+1)\n",
    "\t  X            matrix of training data      (m x input_layer_size)\n",
    "\tOutput:     \n",
    "\t  prediction   label prediction\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tm = np.shape(X)[0]                    # number of training values\n",
    "\tnum_labels = np.shape(Theta2)[0]\n",
    "\t\n",
    "\ta1 = np.hstack((np.ones((m,1)), X))   # add bias (input layer)\n",
    "\ta2 = g(a1 @ Theta1.T)                 # apply sigmoid: input layer --> hidden layer\n",
    "\ta2 = np.hstack((np.ones((m,1)), a2))  # add bias (hidden layer)\n",
    "\ta3 = g(a2 @ Theta2.T)                 # apply sigmoid: hidden layer --> output layer\n",
    "\t\n",
    "\tprediction = np.argmax(a3,1).reshape((m,1))\n",
    "\treturn prediction\n",
    "\n",
    "\n",
    "def reshape(theta, input_layer_size, hidden_layer_size, num_labels):\n",
    "\t\"\"\" reshape theta into Theta1 and Theta2, the weights of our neural network \"\"\"\n",
    "\tncut = hidden_layer_size * (input_layer_size+1)\n",
    "\tTheta1 = theta[0:ncut].reshape(hidden_layer_size, input_layer_size+1)\n",
    "\tTheta2 = theta[ncut:].reshape(num_labels, hidden_layer_size+1)\n",
    "\treturn Theta1, Theta2\n",
    "\t\n",
    "\t\n",
    "def cost_function(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "\t\"\"\" Neural net cost function for a three layer classification network.\n",
    "\tInput:\n",
    "\t  theta               flattened vector of neural net model parameters\n",
    "\t  input_layer_size    size of input layer\n",
    "\t  hidden_layer_size   size of hidden layer\n",
    "\t  num_labels          number of labels\n",
    "\t  X                   matrix of training data\n",
    "\t  y                   vector of training labels\n",
    "\t  lmbda               regularization term\n",
    "\tOutput:\n",
    "\t  J                   cost function\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# unflatten theta\n",
    "\tTheta1, Theta2 = reshape(theta, input_layer_size, hidden_layer_size, num_labels)\n",
    "\t\n",
    "\t# number of training values\n",
    "\tm = len(y)\n",
    "\t\n",
    "\t# Feedforward: calculate the cost function J:\n",
    "\t\n",
    "\ta1 = np.hstack((np.ones((m,1)), X))   \n",
    "\ta2 = g(a1 @ Theta1.T)                 \n",
    "\ta2 = np.hstack((np.ones((m,1)), a2))  \n",
    "\ta3 = g(a2 @ Theta2.T)                 \n",
    "\n",
    "\ty_mtx = 1.*(y==0)\n",
    "\tfor k in range(1,num_labels):\n",
    "\t\ty_mtx = np.hstack((y_mtx, 1.*(y==k)))\n",
    "\n",
    "\t# cost function\n",
    "\tJ = np.sum( -y_mtx * np.log(a3) - (1.0-y_mtx) * np.log(1.0-a3) ) / m\n",
    "\n",
    "\t# add regularization\n",
    "\tJ += lmbda/(2.*m) * (np.sum(Theta1[:,1:]**2)  + np.sum(Theta2[:,1:]**2))\n",
    "\t\n",
    "\treturn J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
=======
>>>>>>> a4eb73592d2a05a800a7862c62e0b72228b438eb
   "outputs": [],
   "source": [
    "def gradient(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):\n",
    "\t\"\"\" Neural net cost function gradient for a three layer classification network.\n",
    "\tInput:\n",
    "\t  theta               flattened vector of neural net model parameters\n",
    "\t  input_layer_size    size of input layer\n",
    "\t  hidden_layer_size   size of hidden layer\n",
    "\t  num_labels          number of labels\n",
    "\t  X                   matrix of training data\n",
    "\t  y                   vector of training labels\n",
    "\t  lmbda               regularization term\n",
    "\tOutput:\n",
    "\t  grad                flattened vector of derivatives of the neural network \n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# unflatten theta\n",
    "\tTheta1, Theta2 = reshape(theta, input_layer_size, hidden_layer_size, num_labels)\n",
    "\t\n",
    "\t# number of training values\n",
    "\tm = len(y)\n",
    "\t\n",
    "\t# Backpropagation: calculate the gradients Theta1_grad and Theta2_grad:\n",
    "\t\n",
    "\tDelta1 = np.zeros((hidden_layer_size,input_layer_size+1))\n",
    "\tDelta2 = np.zeros((num_labels,hidden_layer_size+1))\n",
    "\n",
    "\tfor t in range(m):\n",
    "\t\t\n",
    "\t\t# forward\n",
    "\t\ta1 = X[t,:].reshape((input_layer_size,1))\n",
    "\t\ta1 = np.vstack((1, a1))   #  +bias\n",
    "\t\tz2 = Theta1 @ a1\n",
    "\t\ta2 = g(z2)\n",
    "\t\ta2 = np.vstack((1, a2))   #  +bias\n",
    "\t\ta3 = g(Theta2 @ a2)\n",
    "\t\t\n",
    "\t\t# compute error for layer 3\n",
    "\t\ty_k = np.zeros((num_labels,1))\n",
    "\t\ty_k[y[t,0].astype(int)] = 1\n",
    "\t\tdelta3 = a3 - y_k\n",
    "\t\tDelta2 += (delta3 @ a2.T)\n",
    "\t\t\n",
    "\t\t# compute error for layer 2\n",
    "\t\tdelta2 = (Theta2[:,1:].T @ delta3) * grad_g(z2)\n",
    "\t\tDelta1 += (delta2 @ a1.T)\t\n",
    "\n",
    "\tTheta1_grad = Delta1 / m\n",
    "\tTheta2_grad = Delta2 / m\n",
    "\n",
    "\t# add regularization\n",
    "\tTheta1_grad[:,1:] += (lmbda/m) * Theta1[:,1:]\t\n",
    "\tTheta2_grad[:,1:] += (lmbda/m) * Theta2[:,1:]\n",
    "\n",
    "\t# flatten gradients\n",
    "\tgrad = np.concatenate((Theta1_grad.flatten(), Theta2_grad.flatten()))\n",
    "\n",
    "\treturn grad\n",
    "\n",
    "\n",
    "N_iter = 1\n",
    "J_min = np.Inf\n",
    "theta_best = []\n",
    "Js_train = np.array([])\n",
    "Js_test = np.array([])\n",
    "\n",
    "def callbackF(input_layer_size, hidden_layer_size, num_labels, X, y, lmbda, test, test_label, theta_k):\n",
    "\t\"\"\" Calculate some stats per iteration and update plot \"\"\"\n",
    "\tglobal N_iter\n",
    "\tglobal J_min\n",
    "\tglobal theta_best\n",
    "\tglobal Js_train\n",
    "\tglobal Js_test\n",
    "\t# unflatten theta\n",
    "\tTheta1, Theta2 = reshape(theta_k, input_layer_size, hidden_layer_size, num_labels)\n",
    "\t# training data stats\n",
    "\tJ = cost_function(theta_k, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)\n",
    "\ty_pred = predict(Theta1, Theta2, X)\n",
    "\taccuracy = np.sum(1.*(y_pred==y))/len(y)\n",
    "\tJs_train = np.append(Js_train, J)\n",
    "\t# test data stats\n",
    "\tJ_test = cost_function(theta_k, input_layer_size, hidden_layer_size, num_labels, test, test_label, lmbda)\n",
    "\ttest_pred = predict(Theta1, Theta2, test)\n",
    "\taccuracy_test = np.sum(1.*(test_pred==test_label))/len(test_label)\n",
    "\tJs_test= np.append(Js_test, J_test)\n",
    "\t# print stats\n",
    "\tprint('iter={:3d}:  Jtrain= {:0.4f} acc= {:0.2f}%  |  Jtest= {:0.4f} acc= {:0.2f}%'.format(N_iter, J, 100*accuracy, J_test, 100*accuracy_test))\n",
    "\tN_iter += 1\n",
    "\t# Update theta_best\n",
    "\tif (J_test < J_min):\n",
    "\t\ttheta_best = theta_k\n",
    "\t\tJ_min = J_test\n",
    "\t# Update Plot\n",
    "\titers = np.arange(len(Js_train))\n",
    "\tplt.clf()\n",
    "\tplt.subplot(2,1,1)\n",
    "\tim_size = 32\n",
    "\tpad = 4\n",
    "\tgalaxies_image = np.zeros((3*im_size,6*im_size+2*pad), dtype=int) + 255\n",
    "\tfor i in range(3):\n",
    "\t\tfor j in range(6):\n",
    "\t\t\tidx = 3*j + i + 900*(j>1) + 900*(j>3) + (N_iter % 600) # +10\n",
    "\t\t\tshift = 0 + pad*(j>1) + pad*(j>3)\n",
    "\t\t\tii = i * im_size\n",
    "\t\t\tjj = j * im_size + shift\n",
    "\t\t\tgalaxies_image[ii:ii+im_size,jj:jj+im_size] = X[idx].reshape(im_size,im_size) * 255\n",
    "\t\t\tmy_label = 'E' if y_pred[idx]==0 else 'S' if y_pred[idx]==1 else 'I'\n",
    "\t\t\tmy_color = 'blue' if (y_pred[idx]==y[idx]) else 'red'\n",
    "\t\t\tplt.text(jj+2, ii+10, my_label, color=my_color)\n",
    "\t\t\tif (y_pred[idx]==y[idx]):\n",
    "\t\t\t\tplt.text(jj+4, ii+25, \"✓\", color='blue', fontsize=50)\n",
    "\tplt.imshow(galaxies_image, cmap='gray')\n",
    "\tplt.gca().axis('off')\n",
    "\tplt.subplot(2,1,2)\n",
    "\tplt.plot(iters, Js_test, 'r', label='test')\n",
    "\tplt.plot(iters, Js_train, 'b', label='train')\n",
    "\tplt.xlabel(\"iteration\")\n",
    "\tplt.ylabel(\"cost\")\n",
    "\tplt.xlim(0,600)\n",
    "\tplt.ylim(1,2.1)\n",
    "\tplt.gca().legend()\n",
    "\tplt.pause(0.001)\n",
    "\n",
    "\n",
    "def main():\n",
    "\t\"\"\" Artificial Neural Network for classifying galaxies \"\"\"\n",
    "\t\n",
    "\t# set the random number generator seed\n",
    "\tnp.random.seed(917)\n",
    "\t\n",
    "\t# Load the training and test datasets\n",
    "\ttrain = np.genfromtxt('train.csv', delimiter=',')\n",
    "\ttest = np.genfromtxt('test.csv', delimiter=',')\n",
    "\t\n",
    "\t# get labels (0=Elliptical, 1=Spiral, 2=Irregular)\n",
    "\ttrain_label = train[:,0].reshape(len(train),1)\n",
    "\ttest_label = test[:,0].reshape(len(test),1)\n",
    "\t\n",
    "\t# normalize image data to [0,1]\n",
    "\ttrain = train[:,1:] / 255.\n",
    "\ttest = test[:,1:] / 255.\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iter = 1\n",
    "J_min = np.Inf\n",
    "theta_best = []\n",
    "Js_train = np.array([])\n",
    "Js_test = np.array([])\n",
    "\n",
    "def callbackF(input_layer_size, hidden_layer_size, num_labels, X, y, lmbda, test, test_label, theta_k):\n",
    "\t\"\"\" Calculate some stats per iteration and update plot \"\"\"\n",
    "\tglobal N_iter\n",
    "\tglobal J_min\n",
    "\tglobal theta_best\n",
    "\tglobal Js_train\n",
    "\tglobal Js_test\n",
    "\t# unflatten theta\n",
    "\tTheta1, Theta2 = reshape(theta_k, input_layer_size, hidden_layer_size, num_labels)\n",
    "\t# training data stats\n",
    "\tJ = cost_function(theta_k, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)\n",
    "\ty_pred = predict(Theta1, Theta2, X)\n",
    "\taccuracy = np.sum(1.*(y_pred==y))/len(y)\n",
    "\tJs_train = np.append(Js_train, J)\n",
    "\t# test data stats\n",
    "\tJ_test = cost_function(theta_k, input_layer_size, hidden_layer_size, num_labels, test, test_label, lmbda)\n",
    "\ttest_pred = predict(Theta1, Theta2, test)\n",
    "\taccuracy_test = np.sum(1.*(test_pred==test_label))/len(test_label)\n",
    "\tJs_test= np.append(Js_test, J_test)\n",
    "\t# print stats\n",
    "\tprint('iter={:3d}:  Jtrain= {:0.4f} acc= {:0.2f}%  |  Jtest= {:0.4f} acc= {:0.2f}%'.format(N_iter, J, 100*accuracy, J_test, 100*accuracy_test))\n",
    "\tN_iter += 1\n",
    "\t# Update theta_best\n",
    "\tif (J_test < J_min):\n",
    "\t\ttheta_best = theta_k\n",
    "\t\tJ_min = J_test\n",
    "\t# Update Plot\n",
    "\titers = np.arange(len(Js_train))\n",
    "\tplt.clf()\n",
    "\tplt.subplot(2,1,1)\n",
    "\tim_size = 32\n",
    "\tpad = 4\n",
    "\tgalaxies_image = np.zeros((3*im_size,6*im_size+2*pad), dtype=int) + 255\n",
    "\tfor i in range(3):\n",
    "\t\tfor j in range(6):\n",
    "\t\t\tidx = 3*j + i + 900*(j>1) + 900*(j>3) + (N_iter % 600) # +10\n",
    "\t\t\tshift = 0 + pad*(j>1) + pad*(j>3)\n",
    "\t\t\tii = i * im_size\n",
    "\t\t\tjj = j * im_size + shift\n",
    "\t\t\tgalaxies_image[ii:ii+im_size,jj:jj+im_size] = X[idx].reshape(im_size,im_size) * 255\n",
    "\t\t\tmy_label = 'E' if y_pred[idx]==0 else 'S' if y_pred[idx]==1 else 'I'\n",
    "\t\t\tmy_color = 'blue' if (y_pred[idx]==y[idx]) else 'red'\n",
    "\t\t\tplt.text(jj+2, ii+10, my_label, color=my_color)\n",
    "\t\t\tif (y_pred[idx]==y[idx]):\n",
    "\t\t\t\tplt.text(jj+4, ii+25, \"✓\", color='blue', fontsize=50)\n",
    "\tplt.imshow(galaxies_image, cmap='gray')\n",
    "\tplt.gca().axis('off')\n",
    "\tplt.subplot(2,1,2)\n",
    "\tplt.plot(iters, Js_test, 'r', label='test')\n",
    "\tplt.plot(iters, Js_train, 'b', label='train')\n",
    "\tplt.xlabel(\"iteration\")\n",
    "\tplt.ylabel(\"cost\")\n",
    "\tplt.xlim(0,600)\n",
    "\tplt.ylim(1,2.1)\n",
    "\tplt.gca().legend()\n",
    "\tplt.pause(0.001)\n",
    "\n",
    "\n",
    "def main():\n",
    "\t\"\"\" Artificial Neural Network for classifying galaxies \"\"\"\n",
    "\t\n",
    "\t# set the random number generator seed\n",
    "\tnp.random.seed(917)\n",
    "\t\n",
    "\t# Load the training and test datasets\n",
    "\ttrain = np.genfromtxt('train.csv', delimiter=',')\n",
    "\ttest = np.genfromtxt('test.csv', delimiter=',')\n",
    "\t\n",
    "\t# get labels (0=Elliptical, 1=Spiral, 2=Irregular)\n",
    "\ttrain_label = train[:,0].reshape(len(train),1)\n",
    "\ttest_label = test[:,0].reshape(len(test),1)\n",
    "\t\n",
    "\t# normalize image data to [0,1]\n",
    "\ttrain = train[:,1:] / 255.\n",
    "\ttest = test[:,1:] / 255.\n",
    "\t\n",
    "\t# Construct our data matrix X (2700 x 5000)\n",
    "\tX = train\n",
    "\n",
    "    # Construct our label vector y (2700 x 1)\n",
    "\ty = train_label\n",
    "\t\n",
    "\t# Two layer Neural Network parameters:\n",
    "\tm = np.shape(X)[0]\n",
    "\tinput_layer_size = np.shape(X)[1]\n",
    "\thidden_layer_size = 8\n",
    "\tnum_labels = 3\n",
    "\tlmbda = 1.0    # regularization parameter\n",
    "\t\n",
    "\t# Initialize random weights:\n",
    "\tTheta1 = np.random.rand(hidden_layer_size, input_layer_size+1) * 0.4 - 0.2\n",
    "\tTheta2 = np.random.rand(num_labels, hidden_layer_size+1) * 0.4 - 0.2\n",
    "\t\n",
    "\t# flattened initial guess\n",
    "\ttheta0 = np.concatenate((Theta1.flatten(), Theta2.flatten()))\n",
    "\tJ = cost_function(theta0, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)\n",
    "\tprint('initial cost function J =', J)\n",
    "\ttrain_pred = predict(Theta1, Theta2, train)\n",
    "\tprint('initial accuracy on training set =', np.sum(1.*(train_pred==train_label))/len(train_label))\n",
    "\tglobal Js_train\n",
    "\tglobal Js_test\n",
    "\tJs_train = np.array([J])\n",
    "\tJ_test = cost_function(theta0, input_layer_size, hidden_layer_size, num_labels, test, test_label, lmbda)\n",
    "\tJs_test = np.array([J_test])\n",
    "\n",
    "\t# prep figure\n",
    "\tfig = plt.figure(figsize=(6,6), dpi=80)\n",
    "\n",
    "\t# Minimize the cost function using a nonlinear conjugate gradient algorithm\n",
    "\targs = (input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)  # parameter values\n",
    "\tcbf = partial(callbackF, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda, test, test_label)\n",
    "\ttheta = optimize.fmin_cg(cost_function, theta0, fprime=gradient, args=args, callback=cbf, maxiter=600)\n",
    "\n",
    "\t# unflatten theta\n",
    "\tTheta1, Theta2 = reshape(theta_best, input_layer_size, hidden_layer_size, num_labels)\n",
    "\t\n",
    "\t# Make predictions for the training and test sets\n",
    "\ttrain_pred = predict(Theta1, Theta2, train)\n",
    "\ttest_pred = predict(Theta1, Theta2, test)\n",
    "\t\n",
    "\t# Print accuracy of predictions\n",
    "\tprint('accuracy on training set =', np.sum(1.*(train_pred==train_label))/len(train_label))\n",
    "\tprint('accuracy on test set =', np.sum(1.*(test_pred==test_label))/len(test_label))\t\n",
    "\t\t\t\n",
    "\t# Save figure\n",
    "\tplt.savefig('artificialneuralnetwork.png',dpi=240)\n",
    "\tplt.show()\n",
    "\t    \n",
    "\treturn 0\n",
    "\n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "  main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
=======
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
>>>>>>> a4eb73592d2a05a800a7862c62e0b72228b438eb
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
